{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP (Pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries after verification: 977\n",
      "Caption:  a plot with a plot plot and a plot plot plot\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def clean_caption(caption):\n",
    "    caption = caption.lower()\n",
    "    caption = re.sub(r\"figure\\s+\\d+(\\.\\d+)*\", \"\", caption)\n",
    "    caption = re.sub(r\"[^a-z0-9\\s]\", \"\", caption)\n",
    "    caption = re.sub(r\"\\s+\", \" \", caption).strip()\n",
    "    return caption\n",
    "\n",
    "\n",
    "processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n",
    "model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "captions_df = pd.read_csv('captions.csv') \n",
    "\n",
    "\n",
    "captions_df['clean_caption'] = captions_df['full_caption'].apply(clean_caption)\n",
    "\n",
    "\n",
    "image_folder = 'ImageList'\n",
    "\n",
    "def verify_image_exists(image_id):\n",
    "    return os.path.isfile(os.path.join(image_folder, str(image_id)+\".png\"))\n",
    "\n",
    "captions_df['image_exists'] = captions_df['imageid'].apply(verify_image_exists)\n",
    "captions_df = captions_df[captions_df['image_exists']]\n",
    "\n",
    "print(f\"Total entries after verification: {len(captions_df)}\")\n",
    "\n",
    "\n",
    "def generate_caption(image_path):\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "    output_ids = model.generate(**inputs, max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "\n",
    "    caption = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return caption\n",
    "\n",
    "single_caption = generate_caption(\"ImageList/7.png\")\n",
    "print(\"Caption: \",single_caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption (Pretrained BLIP): figure 2 for the predicted motion of an orbitale at three axes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b9e89e8c0f48f7aae6682571ba0002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  68%|######7   | 671M/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate_caption_blip(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=200,top_k=50,top_p=0.95,do_sample=True)\n",
    "    caption = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "test_image_path = \"ImageList/7.png\"\n",
    "generated_caption = generate_caption_blip(test_image_path)\n",
    "print(f\"Generated Caption (Pretrained BLIP): {generated_caption}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP (Fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "def clean_caption(caption):\n",
    "    caption = caption.lower()\n",
    "    caption = re.sub(r'\\b(figure|fig)\\s*\\d+(\\.\\d+)?', '', caption, flags=re.IGNORECASE)\n",
    "    caption = re.sub(r\"[^a-z0-9A-Z\\s]\", \"\", caption)\n",
    "    caption = re.sub(r\"\\s+\", \" \", caption).strip()\n",
    "    return caption\n",
    "\n",
    "class BlipDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_folder, processor):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data['full_caption'] = self.data['full_caption'].apply(clean_caption)\n",
    "        self.img_folder = img_folder\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.data.iloc[idx]['imageid']\n",
    "        caption = self.data.iloc[idx]['full_caption']\n",
    "        img_path = os.path.join(self.img_folder, f\"{img_id}.png\")\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        inputs = self.processor(image, text=caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        inputs = {key: val.squeeze() for key, val in inputs.items()}\n",
    "        labels = inputs['input_ids'].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        inputs['labels'] = labels\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saikiran/NLP/nlp-torch/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./blip_finetuned\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=1e-5,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='488' max='488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [488/488 06:24, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "dataset = BlipDataset(\"captions.csv\", \"ImageList\", processor)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=lambda data: {\n",
    "        'pixel_values': torch.stack([f['pixel_values'] for f in data]),\n",
    "        'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "        'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
    "        'labels': torch.stack([f['labels'] for f in data])\n",
    "    },\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./blip_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption (Fine-Tuned BLIP): the change in atmospheric co2 emissions of various air and surface systems from 1850 to 1993 and from 1999 and the same air temperature range based on a comparison table of two plots from different air and three panels on a two models of the same line from the four models in the same range are the same scenarios for the changes that have been for each scenario is an uncertain to different scenario\n"
     ]
    }
   ],
   "source": [
    "model = BlipForConditionalGeneration.from_pretrained(\"./blip_finetuned\")\n",
    "model.to(device)\n",
    "\n",
    "generated_caption = generate_caption_blip(test_image_path)\n",
    "print(f\"Generated Caption (Fine-Tuned BLIP): {generated_caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
