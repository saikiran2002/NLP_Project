{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_caption = \"(a) Estimates of the 'scaling factors by which we have to multiply the amplitude of several model-simulated signals to reproduce the corresponding changes in the observed record. The vertical bars indicate the 5 to 95% uncertainty range due to internal variability. A range encompassing unity implies that this combination of forcing amplitude and model-simulated response is consistent with the corresponding observed change, while a range encompassing zero implies that this model-simulated signal is not detectable. Signals are defined as the ensemble mean response to external forcing expressed in large-scale (>5000 km) near-surface temperatures over the 1946 to 1996 period relative to the 1896 to 1996 mean. The first entry (G) shows the scaling factor and 5 to 95% confidence interval obtained if we assume the observations consist only of a response to greenhouse gases plus internal variability. The range is significantly less than one (consistent with results from other models), meaning that models forced with greenhouse gases alone significantly overpredict the observed warming signal. The next eight entries show scaling factors for model-simulated responses to greenhouse and sulphate forcing (GS), with two cases including indirect sulphate and tropospheric ozone forcing, one of these also including stratospheric ozone depletion (GSI and GSIO respectively). All but one (CGCM1) of these ranges is consistent with unity. Hence there is little evidence that models are systematically over- or underpredicting the amplitude of the observed response under the assumption that model-simulated GS signals and internal variability are an adequate representation (i.e. that natural forcing has had little net impact on this diagnostic). Observed residual variability is consistent with this assumption in all but one case (ECHAM3, indicated by the asterisk). We are obliged to make this assumption to include models for which only a simulation of the anthropogenic response is available, but uncertainty estimates in these single-signal cases are incomplete since they do not account for uncertainty in the naturally forced response. These ranges indicate, however, the high level of confidence with which we can reject internal variability as simulated by these various models as an explanation of recent near-surface temperature change. A more complete uncertainty analysis is provided by the next three entries, which show corresponding scaling factors on individual greenhouse (G), sulphate (S), solar-plus-volcanic (N), solar-only (So) and volcanic-only (V) signals for those cases in which the relevant simulations have been performed. In these cases, we estimate multiple factors simultaneously to account for uncertainty in the amplitude of the naturally forced response. The uncertainties increase but the greenhouse signal remains consistently detectable. In one case (ECHAM3) the model appears to be overestimating the greenhouse response (scaling range in the G signal inconsistent with unity), but this result is sensitive to which component of the control is used to define the detection space. It is also not known how it would respond to the inclusion of a volcanic signal. In cases where both solar and volcanic forcing is included (HadCM2 and HadCM3), G and S signals remain detectable and consistent with unity independent of whether natural signals are estimated jointly or separately (allowing for different errors in S and V responses). (b) Estimated contributions to global mean warming over the 20th century, based on the results shown in (a), with 5 to 95% confidence intervals. Although the estimates vary depending on which modelâ€™s signal and what forcing is assumed, and are less certain if more than one signal is estimated, all show a significant contribution from anthropogenic climate change to 20th century warming.\"\n",
    "\n",
    "\n",
    "resnet50_LSTM = \"<start> and g signals from a combination of models with different assumption in s and v signals cases include unity in this chapter except for uncertainty in all cases cases include global global warming potentials for all three cases are consistent with unity hence of sulphate forcing solarplusvolcanic n solaronly so and volcaniconly v signals for those cases in all cases are available in 2030 results from the results for which we can be seen in the text and in the ratio in this document a level of scientific understanding losu index is accorded to each forcing with high medium low and low signals can be seen in the case of these cases the size of these assumption are shown as results from scaling models with unity hence of anthropogenic forcings only all anthropogenic factors are available jointly or separately allowing for all anthropogenic errors in panel c it shows that we cannot be consistent with unity hence of unity hence is consistent with unity hence there is little evidence that we are systematically over that gs factors and what factors is provided in these cases refer to an increase in the corresponding greenhouse gas signal only a sulphate signal for anthropogenic forcing and internal variability a range encompassing unity implies that this combination of forcing amplitude and modelsimulated variability is consistent with unity hence is available but that we hence not include the inclusion of anthropogenic signals from allen et al 2000a end is a models with unity results from\"\n",
    "blip_pretrained_caption = \"figure 2 for the predicted motion of an orbitale at three axes\"\n",
    "blip_generated_caption = \"the change in atmospheric co2 emissions of various air and surface systems from 1850 to 1993 and from 1999 and the same air temperature range based on a comparison table of two plots from different air and three panels on a two models of the same line from the four models in the same range are the same scenarios for the changes that have been for each scenario is an uncertain to different scenario\"\n",
    "gpt_2_pretrained_caption = \"a collage of photos of various types of electronic equipment\"\n",
    "gpt_2_image_caption = \"##ingen estimated technologies blue as a and non different of global sea surface temperature anomalies percentage a hemisphere global mean temperature by the blacks from land for a of land for global mean temperature red and c change and b2 emissions for two to 2100 from different b2 emissions due to 2100 the range of the vertical red temperature degc from the scenario estimates a anomalies percentage lines change to the scenario estimates of the fi in the mean of theal the mean of the mean of the mean are shown from the mean of a et al of the mean in the bars on the mean of the c is a the mean of the climate of the mean of the mean of range of the mean of\"\n",
    "chart_gemma_caption = \"The chart shows the scaling required on model-simulated signals as a function of the 21st century warming (in degrees Celsius) for different climate models. The scaling is measured in units of standard deviation from the global mean, with the black line representing the mean scaling. The chart also shows the estimated contributions to the 21st century warming for different climate models. The contributions are measured in units of standard deviation from the global mean, with the black line representing the mean contribution. The chart shows that the scaling required for the 21st century warming is generally positive, meaning that the models tend to predict higher warming than the global mean. However, there is significant variation in the scaling required across different models. The chart also shows that the estimated contributions to the 21st century warming are generally positive, but the magnitude of the contribution varies across different models. Some interesting findings from the chart include: * The scaling required for the 21st century\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-2 Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "generated_captions = \"The chart shows the scaling required on model-simulated signals as a function of the 21st century warming (in degrees Celsius) for different climate models. The scaling is measured in units of standard deviation from the global mean, with the black line representing the mean scaling. The chart also shows the estimated contributions to the 21st century warming for different climate models. The contributions are measured in units of standard deviation from the global mean, with the black line representing the mean contribution.  The chart shows that the scaling required for the 21st century warming is generally positive, meaning that the models tend to predict higher warming than the global mean. However, there is significant variation in the scaling required across different models. The chart also shows that the estimated contributions to the 21st century warming are generally positive, but the magnitude of the contribution varies across different models. Some interesting findings from the chart include: The scaling required for the 21st century\"\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "num_hypotheses = len(generated_captions)\n",
    "\n",
    "# Tokenize References: replicate the single reference for each hypothesis\n",
    "tokenized_references = [[nltk.word_tokenize(reference_caption.lower())] for _ in range(num_hypotheses)]\n",
    "\n",
    "# Tokenize Hypotheses\n",
    "tokenized_hypotheses = [nltk.word_tokenize(hypo.lower()) for hypo in generated_captions]\n",
    "\n",
    "# Initialize Smoothing Function\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "# Calculate BLEU-2\n",
    "bleu2 = corpus_bleu(\n",
    "    tokenized_references,\n",
    "    tokenized_hypotheses,\n",
    "    weights=(0.9, 0.1, 0, 0),\n",
    "    smoothing_function=smooth_fn\n",
    ")\n",
    "\n",
    "print(f\"BLEU-2 Score: {bleu2 * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/saikiran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/saikiran/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02688172043010753"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "references_tokenized = [nltk.word_tokenize(ref.lower()) for ref in reference_caption]\n",
    "hypothesis_tokenized = nltk.word_tokenize(generated_captions.lower())\n",
    "\n",
    "meteor_score(references_tokenized,hypothesis_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1 Score: 44.50\n",
      "ROUGE-2 F1 Score: 18.98\n",
      "ROUGE-L F1 Score: 17.28\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference_caption, resnet50_LSTM)\n",
    "\n",
    "print(f\"ROUGE-1 F1 Score: {scores['rouge1'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-2 F1 Score: {scores['rouge2'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-L F1 Score: {scores['rougeL'].fmeasure * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0fedeb08834b9291451132997f4912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d76e1d8c9f341ae826b6eb6e284b069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.30 seconds, 3.31 sentences/sec\n",
      "BERTScore Precision: 81.76\n",
      "BERTScore Recall: 80.41\n",
      "BERTScore F1: 81.08\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "P, R, F1 = score([resnet50_LSTM], [reference_caption], lang='en', verbose=True)\n",
    "\n",
    "print(f\"BERTScore Precision: {P.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore Recall: {R.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore F1: {F1.mean() * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP (pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1 Score: 1.65\n",
      "ROUGE-2 F1 Score: 0.00\n",
      "ROUGE-L F1 Score: 1.65\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference_caption, blip_pretrained_caption)\n",
    "\n",
    "print(f\"ROUGE-1 F1 Score: {scores['rouge1'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-2 F1 Score: {scores['rouge2'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-L F1 Score: {scores['rougeL'].fmeasure * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f227f54cc83c47ffb116d07827e0f26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f7ab14a5234db7a94d5358d6152559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.10 seconds, 9.68 sentences/sec\n",
      "BERTScore Precision: 82.39\n",
      "BERTScore Recall: 75.15\n",
      "BERTScore F1: 78.60\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "P, R, F1 = score([blip_pretrained_caption], [reference_caption], lang='en', verbose=True)\n",
    "\n",
    "print(f\"BERTScore Precision: {P.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore Recall: {R.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore F1: {F1.mean() * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP (fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1 Score: 13.47\n",
      "ROUGE-2 F1 Score: 1.50\n",
      "ROUGE-L F1 Score: 9.28\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference_caption, blip_generated_caption)\n",
    "\n",
    "print(f\"ROUGE-1 F1 Score: {scores['rouge1'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-2 F1 Score: {scores['rouge2'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-L F1 Score: {scores['rougeL'].fmeasure * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e61e2cf9a8492f8c0ad4f9d32e2f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf4cf0489244a3b88bcf934c0615d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.09 seconds, 10.73 sentences/sec\n",
      "BERTScore Precision: 81.27\n",
      "BERTScore Recall: 77.23\n",
      "BERTScore F1: 79.20\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "P, R, F1 = score([blip_generated_caption], [reference_caption], lang='en', verbose=True)\n",
    "\n",
    "print(f\"BERTScore Precision: {P.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore Recall: {R.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore F1: {F1.mean() * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT_2_image captioning (pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1 Score: 1.66\n",
      "ROUGE-2 F1 Score: 0.00\n",
      "ROUGE-L F1 Score: 1.66\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference_caption, gpt_2_pretrained_caption)\n",
    "\n",
    "print(f\"ROUGE-1 F1 Score: {scores['rouge1'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-2 F1 Score: {scores['rouge2'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-L F1 Score: {scores['rougeL'].fmeasure * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab01ff4df6b459fa3766b0fb9f6f7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335767fa0a4849ffa9cf2abea0433843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.08 seconds, 12.68 sentences/sec\n",
      "BERTScore Precision: 83.78\n",
      "BERTScore Recall: 74.02\n",
      "BERTScore F1: 78.60\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "P, R, F1 = score([gpt_2_pretrained_caption], [reference_caption], lang='en', verbose=True)\n",
    "\n",
    "print(f\"BERTScore Precision: {P.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore Recall: {R.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore F1: {F1.mean() * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT_2_image captioning (fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1 Score: 22.13\n",
      "ROUGE-2 F1 Score: 4.99\n",
      "ROUGE-L F1 Score: 12.17\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference_caption, gpt_2_image_caption)\n",
    "\n",
    "print(f\"ROUGE-1 F1 Score: {scores['rouge1'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-2 F1 Score: {scores['rouge2'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-L F1 Score: {scores['rougeL'].fmeasure * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab0f689d07d49999e2ae23fe8220c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93d956d31df4a9aa1c4505a43ad0926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.07 seconds, 15.26 sentences/sec\n",
      "BERTScore Precision: 77.73\n",
      "BERTScore Recall: 76.18\n",
      "BERTScore F1: 76.95\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "P, R, F1 = score([gpt_2_image_caption], [reference_caption], lang='en', verbose=True)\n",
    "\n",
    "print(f\"BERTScore Precision: {P.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore Recall: {R.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore F1: {F1.mean() * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1 Score: 26.68\n",
      "ROUGE-2 F1 Score: 5.56\n",
      "ROUGE-L F1 Score: 14.53\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference_caption, chart_gemma_caption)\n",
    "\n",
    "print(f\"ROUGE-1 F1 Score: {scores['rouge1'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-2 F1 Score: {scores['rouge2'].fmeasure * 100:.2f}\")\n",
    "print(f\"ROUGE-L F1 Score: {scores['rougeL'].fmeasure * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c2a7e0753948f09adfb3740d8caaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3a194d97ae4cb1b2c1583ece78ed3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.06 seconds, 15.67 sentences/sec\n",
      "BERTScore Precision: 83.55\n",
      "BERTScore Recall: 79.49\n",
      "BERTScore F1: 81.47\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "P, R, F1 = score([chart_gemma_caption], [reference_caption], lang='en', verbose=True)\n",
    "\n",
    "print(f\"BERTScore Precision: {P.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore Recall: {R.mean() * 100:.2f}\")\n",
    "print(f\"BERTScore F1: {F1.mean() * 100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
